---
---

@inproceedings{DBLP:conf/cvpr/ZhouCGSW18,
  author    = {Zhou*, Lei and Cai*, Chunlei and Gao, Yue and Su, Sanbao and Wu, Junmin},
  title     = {Variational Autoencoder for Low Bit-rate Image Compression},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops},
  year      = {2018},
  abbr      = {CVPR Workshop},
}

@article{DBLP:journals/corr/abs-2003-01595,
  author    = {Gao*, Yue and Rosenberg*, Harrison and Fawaz, Kassem and Jha, Somesh and Hsu, Justin},
  title     = {Analyzing Accuracy Loss in Randomized Smoothing Defenses},
  abstract  = {Recent advances in machine learning (ML) algorithms, especially deep neural networks (DNNs), have demonstrated remarkable success (sometimes exceeding human-level performance) on several tasks, including face and speech recognition. However, ML algorithms are vulnerable to adversarial attacks, such test-time, training-time, and backdoor attacks. In test-time attacks an adversary crafts adversarial examples, which are specially crafted perturbations imperceptible to humans which, when added to an input example, force a machine learning model to misclassify the given input example. Adversarial examples are a concern when deploying ML algorithms in critical contexts, such as information security and autonomous driving. Researchers have responded with a plethora of defenses. One promising defense is randomized smoothing in which a classifierâ€™s prediction is smoothed by adding random noise to the input example we wish to classify. In this paper, we theoretically and empirically explore randomized smoothing. We investigate the effect of randomized smoothing on the feasible hypotheses space, and show that for some noise levels the set of hypotheses which are feasible shrinks due to smoothing, giving one reason why the natural accuracy drops after smoothing. To perform our analysis, we introduce a model for randomized smoothing which abstracts away specifics, such as the exact distribution of the noise. We complement our theoretical results with extensive experiments.},
  journal   = {arXiv},
  year      = {2020},
  abbr      = {Preprint},
  arxiv     = {2003.01595},
}

@inproceedings{scaling,
  title     = {Rethinking Image-Scaling Attacks: The Interplay Between Vulnerabilities in Machine Learning Systems},
  author    = {Gao, Yue and Shumailov, Ilia and Fawaz, Kassem},
  abstract  = {As real-world images come in varying sizes, the machine learning model is part of a larger system that includes an upstream image scaling algorithm. In this paper, we investigate the interplay between vulnerabilities of the image scaling procedure and machine learning models in the decision-based black-box setting. We propose a novel sampling strategy to make a black-box attack exploit vulnerabilities in scaling algorithms, scaling defenses, and the final machine learning model in an end-to-end manner. Based on this scaling-aware attack, we reveal that most existing scaling defenses are ineffective under threat from downstream models. Moreover, we empirically observe that standard black-box attacks can significantly improve their performance by exploiting the vulnerable scaling procedure. We further demonstrate this problem on a commercial Image Analysis API with decision-based black-box attacks.},
  booktitle = {Proceedings of the 39th International Conference on Machine Learning},
  year      = {2022},
  series    = {Proceedings of Machine Learning Research},
  abbr      = {ICML (Oral)},
  note      = {Accepted for Long Presentation},
  code      = {https://github.com/wi-pi/rethinking-image-scaling-attacks},
  arxiv     = {2104.08690}
}

@inproceedings{chen2022practical,
  title={Experimental Security Analysis of the App Model in Business Collaboration Platforms},
  author={Chen*, Yunang and Gao*, Yue and Ceccio, Nick and Chatterjee, Rahul and Fawaz, Kassem and Fernandes, Earlence},
  abstract={Business Collaboration Platforms like Microsoft Teams and Slack enable teamwork by supporting text chatting and third-party resource integration. A user can access online file storage, make video calls, and manage a code repository, all from within the platform, thus making them a hub for sensitive communication and resources. The key enabler for these productivity features is a third-party application model. We contribute an experimental security analysis of this model and the third-party apps. Performing this analysis is challenging because commercial platforms and their apps are closed-source systems. Our analysis methodology is to systematically investigate different types of interactions possible between apps and users. We discover that the access control model in these systems violates two fundamental security principles: least privilege and complete mediation. These violations enable a malicious app to exploit the confidentiality and integrity of user messages and third-party resources connected to the platform. We construct proof-of-concept attacks that can: (1) eavesdrop on user messages without having permission to read those messages; (2) launch fake video calls; (3) automatically merge code into repositories without user approval or involvement. Finally, we provide an analysis of countermeasures that systems like Slack and Microsoft Teams can adopt today.},
  booktitle={31st USENIX Security Symposium},
  abbr={USENIX Security},
  year={2022},
}

@article{pre-processing,
  author    = {Gao, Yue and Shumailov, Ilia and Fawaz, Kassem and Papernot, Nicolas},
  title     = {On the Limitations of Stochastic Pre-processing Defenses},
  abstract  = {Defending against adversarial examples remains an open problem. A common belief is that randomness at inference increases the cost of finding adversarial inputs. An example of such a defense is to apply a random transformation to inputs prior to feeding them to the model. In this paper, we empirically and theoretically investigate such stochastic pre-processing defenses and demonstrate that they are flawed. First, we show that most stochastic defenses are weaker than previously thought; they lack sufficient randomness to withstand even standard attacks like projected gradient descent. This casts doubt on a long-held assumption that stochastic defenses invalidate attacks designed to evade deterministic defenses and force attackers to integrate the Expectation over Transformation (EOT) concept. Second, we show that stochastic defenses confront a trade-off between adversarial robustness and model invariance; they become less effective as the defended model acquires more invariance to their randomization. Future work will need to decouple these two effects. Our code is available in the supplementary material.},
  journal   = {arXiv},
  year      = {2022},
  abbr      = {Preprint},
  arxiv     = {2206.09491},
}
